搭建CNN框架：
nn.Module是非常常见的自定义类，所有模型的基类。
先是一个卷积，再是一个ReLU激活函数，最后再来个最大池化。
池化：就是给定一个矩形（小一点的），然后通过一定的移动方式，在这个区间上找最大值，然后不重复的这样遍历完所有的像素点，得到一张新图（新的张量）。
所以，在注释的地方表示的是到这里的时候这个数据的shape，
通过.view(shape(0),-1) 这样的方式，将数据展开为一个一维向量。
之后，再通过一个线性函数，转成10个线性函数的结果来，表示对应10个数字的概率（其实可能是负数的，但是会逐渐的改善，大致上能拟合出结果来）

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.layer1 = nn.Sequential(
            # (1, 28, 28)
            nn.Conv2d(
                in_channels=1,
                out_channels=16,
                kernel_size=5,  # 卷积filter, 移动块长
                stride=1,  # filter的每次移动步长
                padding=2,
                groups=1
            ),
            # (16, 28, 38)
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
            # (16, 14, 14)
        )
        self.layer2 = nn.Sequential(

            nn.Conv2d(
                in_channels=16,
                out_channels=32,
                kernel_size=5,
                stride=1,
                padding=2
            ),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        self.layer3 = nn.Linear(32 * 7 * 7, 10)

    def forward(self, x):
        # print(x.shape)
        x = self.layer1(x)
        # print(x.shape)
        x = self.layer2(x)
        # print(x.shape)
        x = x.view(x.size(0), -1)
        # print(x.shape)
        x = self.layer3(x)
        # print(x.shape)
        return x


# 自己搭的第一个CNN
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
input = torch.randn(1, 1, 32, 32)
output = net(input)
output.backward(torch.randn(1, 10), retain_graph=True)
# loss function
target = torch.randn(10)
target = target.view(1, -1)
criterion = nn.MSELoss()
loss = criterion(output, target)
# Update the weights
optimizer = optim.SGD(net.parameters(), lr=0.01)
optimizer.zero_grad()
loss.backward()
optimizer.step()
print(loss)
