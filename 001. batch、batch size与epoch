batch是批。
深度学习每一次参数的更新所需要损失函数并不是由一个{data: label}获得的，而是由一组数据加权得到的，这一组数据的数量就是[batch size]。
batch的思想，至少有两个作用:
一是更好的处理非凸的损失函数，非凸的情况下，全样本就算工程上算的动，也会卡在局部优上， 
batch表示了全样本的部分抽样实现，相当于人为引入修正梯度上的采样噪声，使“一路不通找别路”更有可能搜索最优值；
二是合理利用内存容量。
如果数据集较小，可以采用全数据集（Full batch learning）的形式，
这样有两个显然的好处：
1.由全数据集计算的梯度能够更好的代表总体，从而更准确的朝向极值所在的方向；
2.不同权重的梯度值差别很大，因此选取一个全局的学习率会比较困难（？）

batch size最大是样本总数N，此时就是Full batch learning；
最小是1，即每次只训练一个样本，这就是在线学习（Online Learning）。

当我们分批学习时，每次使用过全部训练数据完成一次Forword运算以及一次BP运算，成为完成了一次epoch。
